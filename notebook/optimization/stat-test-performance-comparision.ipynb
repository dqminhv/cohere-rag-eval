{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_csv_to_dataframe(file_path):\n",
    "  \"\"\"Reads a CSV file and returns it as a Pandas DataFrame.\n",
    "\n",
    "  Args:\n",
    "    file_path: The path to the CSV file.\n",
    "\n",
    "  Returns:\n",
    "    A Pandas DataFrame containing the data from the CSV file.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "  except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: The CSV file at {file_path} is empty.\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error: An unexpected error occurred while reading the CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere query translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from from_root import from_root\n",
    "import os\n",
    "\n",
    "baseline_path = os.path.join(from_root(), \"data-test/test_dataset/test_dataset_it_baseline.csv\")\n",
    "\n",
    "cohere_query_tran_path = os.path.join(from_root(), \"data-test/test_dataset/test_dataset_it_with_cohere_query_translate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = read_csv_to_dataframe(baseline_path)\n",
    "df_cohere_query_tran = read_csv_to_dataframe(cohere_query_tran_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.723383916545185\n",
      "Fail to reject the null hypothesis. There is no significant difference in answer relevanacy between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['answer_relevancy'].tolist()  # New model scores\n",
    "cohere_query_tran_scores = df_cohere_query_tran['answer_relevancy'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(cohere_query_tran_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in answer relevanacy between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in answer relevanacy between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.5649865965151069\n",
      "Fail to reject the null hypothesis. There is no significant difference in faithfulness between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['faithfulness'].tolist()  # New model scores\n",
    "cohere_query_tran_scores = df_cohere_query_tran['faithfulness'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(cohere_query_tran_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in faithfulness between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in faithfulness between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 1.0\n",
      "Fail to reject the null hypothesis. There is no significant difference in context_recall between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['context_recall'].tolist()  # New model scores\n",
    "cohere_query_tran_scores = df_cohere_query_tran['context_recall'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(cohere_query_tran_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in context_recall between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in context_recall between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conext Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.37406279749088245\n",
      "Fail to reject the null hypothesis. There is no significant difference in context_precision between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['context_precision'].tolist()  # New model scores\n",
    "cohere_query_tran_scores = df_cohere_query_tran['context_precision'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(cohere_query_tran_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in context_precision between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in context_precision between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from from_root import from_root\n",
    "import os\n",
    "\n",
    "baseline_path = os.path.join(from_root(), \"data-test/test_dataset/test_dataset_it_baseline.csv\")\n",
    "\n",
    "hyde_path = os.path.join(from_root(), \"data-test/test_dataset/test_dataset_it_with_hyde.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = read_csv_to_dataframe(baseline_path)\n",
    "df_hyde = read_csv_to_dataframe(hyde_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.627029093446337\n",
      "Fail to reject the null hypothesis. There is no significant difference in answer relevanacy between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['answer_relevancy'].tolist()  # New model scores\n",
    "hyde_scores = df_hyde['answer_relevancy'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(hyde_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in answer relevanacy between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in answer relevanacy between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 1.0\n",
      "Fail to reject the null hypothesis. There is no significant difference in faithfulness between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['faithfulness'].tolist()  # New model scores\n",
    "hyde_scores = df_hyde['faithfulness'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(hyde_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in faithfulness between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in faithfulness between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 1.0\n",
      "Fail to reject the null hypothesis. There is no significant difference in context_recall between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['context_recall'].tolist()  # New model scores\n",
    "hyde_scores = df_hyde['context_recall'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(hyde_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in context_recall between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in context_recall between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conext Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 4.657054587626058e-05\n",
      "Reject the null hypothesis. There is a significant difference in context_precision between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['context_precision'].tolist()  # Baseline model scores\n",
    "hyde_scores = df_hyde['context_precision'].tolist()  # New model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(hyde_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in context_precision between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in context_precision between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from from_root import from_root\n",
    "import os\n",
    "\n",
    "baseline_path = os.path.join(from_root(), \"data-test/test_dataset/test_dataset_it_baseline.csv\")\n",
    "\n",
    "multi_query_path = os.path.join(from_root(), \"data-test/test_dataset/test_dataset_it_multi_query.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = read_csv_to_dataframe(baseline_path)\n",
    "df_multi_query = read_csv_to_dataframe(multi_query_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 1.0\n",
      "Fail to reject the null hypothesis. There is no significant difference in answer relevanacy between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['answer_relevancy'].tolist()  # Baseline model scores\n",
    "multi_query_scores = df_multi_query['answer_relevancy'].tolist()  # New model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(multi_query_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in answer relevanacy between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in answer relevanacy between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.4794748796063637\n",
      "Fail to reject the null hypothesis. There is no significant difference in faithfulness between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['faithfulness'].tolist()  # New model scores\n",
    "multi_query_scores = df_multi_query['faithfulness'].tolist()  # Baseline model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(multi_query_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in faithfulness between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in faithfulness between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 1.0\n",
      "Fail to reject the null hypothesis. There is no significant difference in context_recall between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['context_recall'].tolist()  # Baseline model scores\n",
    "multi_query_scores = df_multi_query['context_recall'].tolist()  # New model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(multi_query_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in context_recall between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in context_recall between the two models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conext Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.37406279749088245\n",
      "Fail to reject the null hypothesis. There is no significant difference in context_precision between the two models.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming model1_scores and model2_scores are your lists of performance scores\n",
    "baseline_score = df_baseline['context_precision'].tolist()  # Baseline model scores\n",
    "multi_query_scores = df_multi_query['context_precision'].tolist()  # New model scores\n",
    "\n",
    "# Perform the Mann-Whitney U Test\n",
    "statistic, p_value = stats.mannwhitneyu(multi_query_scores, baseline_score)\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if p-value is less than alpha\n",
    "if p_value < alpha:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Reject the null hypothesis. There is a significant difference in context_precision between the two models.\")\n",
    "else:\n",
    "    print(\"p-value: {}\".format(p_value))\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant difference in context_precision between the two models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
