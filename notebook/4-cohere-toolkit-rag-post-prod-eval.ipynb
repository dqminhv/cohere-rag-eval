{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a real-world scenario, we fed the system a diverse set of questions that would typically arise from multiple users in different conversations. We then utilized the 2-chat-history-extraction.ipynb notebook to extract the system's responses.\n",
    "\n",
    "Since these questions were generated organically and don't have predefined ground truths, we focused our evaluation on two key RAGAS metrics: answer relevancy and faithfulness. This approach provides insights into how well the system can address user queries and maintain consistency in its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "#pip install -U python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')\n",
    "from from_root import from_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def serialize_list(value):\n",
    "    \"\"\"Serializes a list to a JSON string.\"\"\"\n",
    "    return json.dumps(value)\n",
    "\n",
    "def deserialize_list(value):\n",
    "    \"\"\"Deserializes a JSON string back into a list.\"\"\"\n",
    "    return json.loads(value)\n",
    "\n",
    "def save_dataframe_with_list_column(df, filename):\n",
    "    \"\"\"Saves a DataFrame with a list column to a CSV file, preserving the list structure.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to save.\n",
    "        filename: The name of the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the serialization function to the list column\n",
    "    df['contexts'] = df['contexts'].apply(serialize_list)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_dataframe_with_list_column(filename):\n",
    "    \"\"\"Loads a DataFrame from a CSV file, restoring the list structure.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        The loaded DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the DataFrame\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Apply the deserialization function to the list column\n",
    "    df['contexts'] = df['contexts'].apply(deserialize_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data from the chat history extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from from_root import from_root\n",
    "file_name = \"test_dataset_it_openai_deployment_test.csv\"\n",
    "df_question_answer_contexts = load_dataframe_with_list_column(os.path.join(from_root(), \"data-test/test-dataset/\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_answer_contexts = df_question_answer_contexts[df_question_answer_contexts['conversation_id']=='8117578e-d06a-4bfd-988f-b2eee28121f1'][['question', 'answer', 'contexts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to RAGAS format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let evaluate the first 10 system's responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "question = list(df_question_answer_contexts['question'])\n",
    "answer = list(df_question_answer_contexts['answer'])\n",
    "contexts = list(df_question_answer_contexts['contexts'])\n",
    "\n",
    "data_samples = {\n",
    "    'question': question,\n",
    "    'answer': answer,\n",
    "    'contexts': contexts,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "# from ragas.integrations.langsmith import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    #context_recall,\n",
    "    #context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09238de4c5b94a7bbd0d028ee19791ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        #context_recall,\n",
    "        #context_precision,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"eval_result_post_prod_test_dataset_it_openai_deployment.csv\"\n",
    "json_file_path = os.path.join(from_root(), \"data-test/eval-result/\", file_name)\n",
    "result.to_pandas().to_csv(json_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
