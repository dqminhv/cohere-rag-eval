{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the Evaluations with LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .env file\n",
    "#pip install -U python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pymongo import MongoClient\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI()\n",
    "embedding_model=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ATLAS_CONNECTION_STRING\"] = os.getenv(\"ATLAS_CONNECTION_STRING\")\n",
    "client = MongoClient(os.environ[\"ATLAS_CONNECTION_STRING\"])\n",
    "db_name = \"tech_innovators_db\"\n",
    "collection_name = \"tech_innovators_collection\"\n",
    "atlas_collection = client[db_name][collection_name]\n",
    "index_name = \"vector_index_erp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store_retriver(index_name, embedding_model, collection):\n",
    "\n",
    "  vector_store = MongoDBAtlasVectorSearch(\n",
    "      embedding = embedding_model,\n",
    "      collection = atlas_collection,\n",
    "      index_name = index_name\n",
    "  )\n",
    "\n",
    "  retriever = vector_store.as_retriever(\n",
    "      search_type = \"similarity\",\n",
    "      search_kwargs = { \"k\": 10 }\n",
    "  )\n",
    "\n",
    "  return(vector_store, retriever)\n",
    "\n",
    "vector_store, retriever = get_vector_store_retriver(\"vector_index_erp\", embedding_model, atlas_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Atlas Vector Search as a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "   search_type = \"similarity\",\n",
    "   search_kwargs = { \"k\": 10 }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "template = \"\"\"\n",
    "Imagine you are an expert in corporate assistant and try to answer the below question\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "   return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Construct a chain to answer questions on your data\n",
    "rag_chain = (\n",
    "   { \"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "   | custom_rag_prompt\n",
    "   | llm\n",
    "   | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=retriever, return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory function that return a new qa chain\n",
    "def create_qa_chain(return_context=True):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vector_store.as_retriever(search_type = \"similarity\",search_kwargs = { \"k\": 10 }),\n",
    "        return_source_documents=return_context,\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a test dataset in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing dataset:  hr test\n"
     ]
    }
   ],
   "source": [
    "# dataset creation\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"hr test\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    # if not create a new one with the generated query examples\n",
    "    #dataset = client.create_dataset(\n",
    "    #    dataset_name=dataset_name, description=\"HR department test dataset\"\n",
    "    #)\n",
    "    #for q in question:\n",
    "    #   client.create_example(\n",
    "    #        inputs={\"query\": q},\n",
    "    #       dataset_id=dataset.id,\n",
    "    #    )\n",
    "    print(\"No dataset exist: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAGAS evaluation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# create evaluation chains\n",
    "faithfulness_chain = EvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = EvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = EvaluatorChain(metric=context_precision)\n",
    "context_recall_chain = EvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1353: LangChainPendingDeprecationWarning: The input_mapper argument is deprecated and will be removed in a future release. Please add a  RunnableLambda to your chain to map inputs to the expected format instead. Example:\n",
      "def construct_chain():\n",
      "    my_chain = ...\n",
      "    input_mapper = {'other_key': 'MyOtherInput', 'my_input_key': x}\n",
      "    return input_mapper | my_chain\n",
      "run_on_dataset(..., llm_or_chain_factory=construct_chain)\n",
      "(See https://api.python.langchain.com/en/latest/schema/langchain.schema.runnable.base.RunnableLambda.html)\n",
      "  warn_deprecated(\"0.0.305\", message=_INPUT_MAPPER_DEP_WARNING, pending=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Database' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunEvalConfig, run_on_dataset\n\u001b[0;32m      3\u001b[0m evaluation_config \u001b[38;5;241m=\u001b[39m RunEvalConfig(\n\u001b[0;32m      4\u001b[0m     custom_evaluators\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      5\u001b[0m         faithfulness_chain,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     prediction_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_qa_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1374\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[1;34m(client, dataset_name, llm_or_chain_factory, evaluation, dataset_version, concurrency_level, project_name, project_metadata, verbose, revision_id, **kwargs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     warn_deprecated(\n\u001b[0;32m   1367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1368\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following arguments are deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1372\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m Client()\n\u001b[1;32m-> 1374\u001b[0m container \u001b[38;5;241m=\u001b[39m \u001b[43m_DatasetRunContainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concurrency_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1388\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1389\u001b[0m         _run_llm_or_chain(\n\u001b[0;32m   1390\u001b[0m             example,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1395\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(container\u001b[38;5;241m.\u001b[39mexamples, container\u001b[38;5;241m.\u001b[39mconfigs)\n\u001b[0;32m   1396\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:1178\u001b[0m, in \u001b[0;36m_DatasetRunContainer.prepare\u001b[1;34m(cls, client, dataset_name, llm_or_chain_factory, project_name, evaluation, tags, input_mapper, concurrency_level, project_metadata, revision_id, dataset_version)\u001b[0m\n\u001b[0;32m   1176\u001b[0m         project_metadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1177\u001b[0m     project_metadata\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: revision_id})\n\u001b[1;32m-> 1178\u001b[0m wrapped_model, project, dataset, examples \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_eval_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1187\u001b[0m tags \u001b[38;5;241m=\u001b[39m tags \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (project\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32md:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:977\u001b[0m, in \u001b[0;36m_prepare_eval_run\u001b[1;34m(client, dataset_name, llm_or_chain_factory, project_name, project_metadata, tags, dataset_version)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_eval_run\u001b[39m(\n\u001b[0;32m    968\u001b[0m     client: Client,\n\u001b[0;32m    969\u001b[0m     dataset_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    974\u001b[0m     dataset_version: Optional[Union[\u001b[38;5;28mstr\u001b[39m, datetime]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    975\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[MCF, TracerSession, Dataset, List[Example]]:\n\u001b[0;32m    976\u001b[0m     wrapped_model \u001b[38;5;241m=\u001b[39m _wrap_in_chain_factory(llm_or_chain_factory, dataset_name)\n\u001b[1;32m--> 977\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m     examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(client\u001b[38;5;241m.\u001b[39mlist_examples(dataset_id\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid, as_of\u001b[38;5;241m=\u001b[39mdataset_version))\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m examples:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Database' object is not callable"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators=[\n",
    "        faithfulness_chain,\n",
    "        answer_rel_chain,\n",
    "        context_rel_chain,\n",
    "        context_recall_chain,\n",
    "    ],\n",
    "    prediction_key=\"result\",\n",
    ")\n",
    "\n",
    "result = run_on_dataset(\n",
    "    client,\n",
    "    dataset_name,\n",
    "    create_qa_chain,\n",
    "    evaluation=evaluation_config,\n",
    "    input_mapper=lambda x: x,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation on LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"answer\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda _: [\"\"]),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness\n",
    "from ragas.integrations.langsmith import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'just_llm_1' at:\n",
      "https://smith.langchain.com/o/08bc9556-81b3-56d7-98aa-4f87d6cdfca5/datasets/f04f14f3-f165-48c3-8d94-dbf759844c7d/compare?selectedSessions=3a32c6dc-6e03-4644-9502-6312370d0c8b\n",
      "\n",
      "View all tests for Dataset hr test at:\n",
      "https://smith.langchain.com/o/08bc9556-81b3-56d7-98aa-4f87d6cdfca5/datasets/f04f14f3-f165-48c3-8d94-dbf759844c7d\n",
      "[>                                                 ] 0/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run 807be56d-fc72-441f-95ea-3c2634e8a2c3 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_2'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_2'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_2'.\")\n",
      "Error evaluating run 9554dfe2-be25-4995-af38-8fd3743c0ad0 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_3'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_3'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_3'.\")\n",
      "Error evaluating run 82276ec0-037e-4ae6-a10a-f231f47bd712 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_0'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_0'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_0'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------->                                 ] 3/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run a91cf5be-4187-4097-9912-daea18202b74 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_1'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_1'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_1'.\")\n",
      "Error evaluating run 9a0a9f54-8455-440d-b4c1-912a4dfa91eb with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_4'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_4'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_4'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------->                      ] 5/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run c1fb5128-3f57-4a8b-9dd4-6c8c75899feb with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_2'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_2'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_2'.\")\n",
      "Error evaluating run 431294b1-8d86-4396-8f04-363a55a68c10 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_1'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_1'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_1'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------------->           ] 7/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run f57bae72-f8b1-4221-b9b7-397e2fd595c6 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_0'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_0'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_0'.\")\n",
      "Error evaluating run 0721a994-a712-4f12-81d7-af90597e2e91 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_3'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-4_3'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-4_3'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 9/9"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82276ec0-037e-4ae6-a10a-f231f47bd712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.940501</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.180954</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666629</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.968467</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.107662</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.209656</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       error  execution_time                                run_id\n",
       "count      0        9.000000                                     9\n",
       "unique     0             NaN                                     9\n",
       "top      NaN             NaN  82276ec0-037e-4ae6-a10a-f231f47bd712\n",
       "freq     NaN             NaN                                     1\n",
       "mean     NaN        0.940501                                   NaN\n",
       "std      NaN        0.180954                                   NaN\n",
       "min      NaN        0.666629                                   NaN\n",
       "25%      NaN        0.785931                                   NaN\n",
       "50%      NaN        0.968467                                   NaN\n",
       "75%      NaN        1.107662                                   NaN\n",
       "max      NaN        1.209656                                   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"hr test\"\n",
    "# evaluate rag_chain\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm,\n",
    "    experiment_name=\"just_llm_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
