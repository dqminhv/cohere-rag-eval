{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=os.getenv(\"DEFAULT_OPENAI_MODEL\")) # DEFAULT_OPENAI_MODEL='gpt-4o-mini-2024-07-18'\n",
    "\n",
    "# embedding_model=OpenAIEmbeddings(model=os.getenv(\"DEFAULT_OPENAI_EMBEDDING\"), disallowed_special=())\n",
    "embedding_model=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MongoDB vector database\n",
    "client = MongoClient(os.getenv(\"ATLAS_CONNECTION_STRING\"))\n",
    "db_name = \"fellowshipai\"\n",
    "collection_name = \"enterprise_data\"\n",
    "atlas_collection = client[db_name][collection_name]\n",
    "index_name = \"vector_index_erp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector store and retriever\n",
    "def get_vector_store_retriver(index_name, embedding_model, collection):\n",
    "\n",
    "  vector_store = MongoDBAtlasVectorSearch(\n",
    "      embedding = embedding_model,\n",
    "      collection = atlas_collection,\n",
    "      index_name = index_name\n",
    "  )\n",
    "\n",
    "  retriever = vector_store.as_retriever(\n",
    "      search_type = \"similarity\",\n",
    "      search_kwargs = { \"k\": 10}  # \"score_threshold\": 0.75 \n",
    "  )\n",
    "\n",
    "  return(vector_store, retriever)\n",
    "\n",
    "vector_store, retriever = get_vector_store_retriver(\"vector_index_erp\", embedding_model, atlas_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Atlas Vector Search as a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = { \"k\": 10  }\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cohere numpy\n",
    "\n",
    "import numpy as np\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\")) # Get your API key: https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere  # Assuming you have Cohere library installed\n",
    "\n",
    "def get_translated_queries(query_string):\n",
    "  \"\"\"Gets a query string and returns translated search queries using Cohere.\n",
    "\n",
    "  Args:\n",
    "      query_string (str): The user's original query.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of translated search queries.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize Cohere client (replace with your API key)\n",
    "  co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "  # Send the query with search_queries_only parameter\n",
    "  response = co.chat(message=query_string, search_queries_only=True)\n",
    "  query_optimized = response.search_queries[0].text\n",
    "\n",
    "  # Extract translated search queries\n",
    "  # translated_queries = []\n",
    "  if response.search_queries:\n",
    "    # for query in response.search_queries:\n",
    "    # translated_queries.append(query.text)\n",
    "    return query_optimized\n",
    "  else:\n",
    "    print(\"Cohere did not generate any search queries for this input.\")\n",
    "    return query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"How to stay connected with the company and do you organize team events?\"\n",
    "translated_queries = get_translated_queries(query)\n",
    "print(translated_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_embedding(query, model=\"embed-english-v3.0\"):\n",
    "  \"\"\"Gets the embedding for a given query using Cohere's embedding API.\n",
    "\n",
    "  Args:\n",
    "      query (str): The query to be embedded.\n",
    "      model (str, optional): The Cohere embedding model to use. Defaults to \"embed-english-v3.0\".\n",
    "\n",
    "  Returns:\n",
    "      numpy.ndarray: The embedding vector for the query.\n",
    "  \"\"\"\n",
    "  # Initialize Cohere client (replace with your API key)\n",
    "  co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "  # Embed the query\n",
    "  embedding = co.embed(\n",
    "      model=model,\n",
    "      input_type=\"search_query\",\n",
    "      texts=[query]).embeddings\n",
    "\n",
    "  return embedding  # Return the embedding for the first query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatCohere\n",
    "\n",
    "# Define a prompt template\n",
    "import pprint\n",
    "def call_cohere(question):\n",
    "\n",
    "   question = question['question']\n",
    "   query = get_translated_queries(question)\n",
    "   query_embedded = get_query_embedding(query)\n",
    "\n",
    "   retriever = vector_store.as_retriever(\n",
    "      search_type = \"similarity\",\n",
    "      search_kwargs = { \"k\": 10  }\n",
    "      )\n",
    "\n",
    "   preamble = \"\" # read from cohere front end or use the input to the API\n",
    "   #question = \n",
    "   SAFETY_PREAMBLE = \"The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\"\n",
    "   BASIC_RULES = \"You are a powerful conversational AI trained by openAI to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\"\n",
    "   TASK_CONTEXT = \"You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\"\n",
    "   STYLE_GUIDE = \"Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\"\n",
    "   INSTRUCTIONS = \"\"\"You are an enterprise Chatbot, an AI assistant designed to retrieve information from the enterprise Confluence system. \n",
    "   You specialize in providing accurate answers related to various departments like Marketing, IT, HR, Finance, and Corporate Communications. \n",
    "               Use the following pieces of context to answer the question at the end.\n",
    "               If you don't know the answer, just say that you don't know, don't try to make up an answer\n",
    "               {context}\n",
    "         \"\"\"\n",
    "         \n",
    "   template = f\"\"\"\n",
    "\n",
    "      {SAFETY_PREAMBLE}\n",
    "      {BASIC_RULES}\n",
    "      {TASK_CONTEXT}\n",
    "      {STYLE_GUIDE}\n",
    "      {INSTRUCTIONS}\n",
    "\n",
    "   \"\"\"\n",
    "   if preamble:\n",
    "      template += f\"\"\"{preamble}\\n\\n\"\"\"\n",
    "\n",
    "\n",
    "   template +=  f\"\"\"Question: {query}\\n\\n\"\"\"\n",
    "\n",
    "   custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "   #llm = get_llm_model(\"openai\")\n",
    "   # llm = ChatOpenAI(model=os.getenv(\"DEFAULT_OPENAI_MODEL\"))\n",
    "   \n",
    "   llm = ChatCohere(cohere_api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "\n",
    "   def format_docs(docs):\n",
    "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "   # Construct a chain to answer questions on your data\n",
    "   rag_chain = (\n",
    "      { \"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "      | custom_rag_prompt\n",
    "      | llm\n",
    "      | StrOutputParser()\n",
    "   )\n",
    "\n",
    "   # Prompt the chain\n",
    "   query = query\n",
    "   answer = rag_chain.invoke(query)\n",
    "   similar = retriever.invoke(query_embedded)\n",
    "\n",
    "\n",
    "   return{\n",
    "      'answer': answer,\n",
    "      'contexts': [str(doc) for doc in similar]\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Test sample\n",
    "question = {'question': \"How does a supportive culture impact employee engagement and align with Tech Innovators Inc.'s approach to employment relations and engagement?\"}\n",
    "answer = call_cohere(question)\n",
    "print(answer['answer'][:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"page_content='incidents and vulnerabilities.Automated playbooks in Azure Sentinel must be used for incident response and remediation.7.2 Incident ManagementA cloud-specific incident response plan must be developed' metadata={'_id': '66d8163b0533e009aa2d4ad1', 'pageid': '851993', 'department': 'IT', 'title': 'Tech Innovators Inc. Azure Cloud Services Security Policy'}\", \"page_content='incidents and vulnerabilities.Automated playbooks in Azure Sentinel must be used for incident response and remediation.7.2 Incident ManagementA cloud-specific incident response plan must be developed' metadata={'_id': '66d816560533e009aa2d4cf6', 'pageid': '851993', 'department': 'IT', 'title': 'Tech Innovators Inc. Azure Cloud Services Security Policy'}\", \"page_content='incidents and vulnerabilities.Automated playbooks in Azure Sentinel must be used for incident response and remediation.7.2 Incident ManagementA cloud-specific incident response plan must be developed' metadata={'_id': '66d816720533e009aa2d4f1b', 'pageid': '851993', 'department': 'IT', 'title': 'Tech Innovators Inc. Azure Cloud Services Security Policy'}\", \"page_content='incidents and vulnerabilities.Automated playbooks in Azure Sentinel must be used for incident response and remediation.7.2 Incident ManagementA cloud-specific incident response plan must be developed' metadata={'_id': '66d8168e0533e009aa2d5140', 'pageid': '851993', 'department': 'IT', 'title': 'Tech Innovators Inc. Azure Cloud Services Security Policy'}\", \"page_content='on the CV Parser Software link.Choose the version compatible with your operating system.Click Download.\\xa0Installing the SoftwareLocate the downloaded installer file in your Downloads' metadata={'_id': '66d8168b0533e009aa2d5110', 'pageid': '655380', 'department': 'IT', 'title': 'Tech Innovator CV Parser Installation Guide'}\", \"page_content='on the CV Parser Software link.Choose the version compatible with your operating system.Click Download.\\xa0Installing the SoftwareLocate the downloaded installer file in your Downloads' metadata={'_id': '66d816380533e009aa2d4aa1', 'pageid': '655380', 'department': 'IT', 'title': 'Tech Innovator CV Parser Installation Guide'}\", \"page_content='on the CV Parser Software link.Choose the version compatible with your operating system.Click Download.\\xa0Installing the SoftwareLocate the downloaded installer file in your Downloads' metadata={'_id': '66d816540533e009aa2d4cc6', 'pageid': '655380', 'department': 'IT', 'title': 'Tech Innovator CV Parser Installation Guide'}\", \"page_content='on the CV Parser Software link.Choose the version compatible with your operating system.Click Download.\\xa0Installing the SoftwareLocate the downloaded installer file in your Downloads' metadata={'_id': '66d816700533e009aa2d4eeb', 'pageid': '655380', 'department': 'IT', 'title': 'Tech Innovator CV Parser Installation Guide'}\", \"page_content='internal training programs?A3: Internal training programs are announced via the company’s internal communication channels, such as the intranet and email. To apply, follow the instructions provided' metadata={'_id': '66d816450533e009aa2d4bd1', 'pageid': '884753', 'department': 'HR', 'title': 'Tech Innovators Inc. Employee Frequently Asked Questions (FAQs)'}\", \"page_content='internal training programs?A3: Internal training programs are announced via the company’s internal communication channels, such as the intranet and email. To apply, follow the instructions provided' metadata={'_id': '66d816600533e009aa2d4df6', 'pageid': '884753', 'department': 'HR', 'title': 'Tech Innovators Inc. Employee Frequently Asked Questions (FAQs)'}\"]\n"
     ]
    }
   ],
   "source": [
    "print(answer['contexts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG pipeline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data set prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def json_to_dataframe(json_file_path):\n",
    "  \"\"\"Reads a JSON file and converts it to a pandas DataFrame.\n",
    "\n",
    "  Args:\n",
    "    json_file_path (str): The path to the JSON file.\n",
    "\n",
    "  Returns:\n",
    "    pandas.DataFrame: The DataFrame created from the JSON data.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "  # Handle different JSON structures\n",
    "  if isinstance(data, list):\n",
    "    # If the JSON data is a list of dictionaries, create a DataFrame directly\n",
    "    df = pd.DataFrame(data)\n",
    "  elif isinstance(data, dict):\n",
    "    # If the JSON data is a single dictionary, convert it to a list of dictionaries\n",
    "    df = pd.DataFrame([data])\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported JSON structure\")\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from from_root import from_root\n",
    "import os\n",
    "folder = \"data-test/test_dataset/test_dataset_hr.json\"\n",
    "json_file_path = os.path.join(from_root(), folder)\n",
    "data_to_upload = json_to_dataframe(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the answers for the questions in the dataset\n",
    "# examples = client.list_examples(dataset_name=\"hr test\")\n",
    "answers = []\n",
    "for question in data_to_upload['question']:\n",
    "    question_dict = {'question': question}\n",
    "    answer = call_openai(question_dict)\n",
    "    answers.append(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the dataset with answers\n",
    "data_to_upload['answers'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "question = list(data_to_upload['question'])\n",
    "answer = list(data_to_upload['answers'])\n",
    "contexts = list(data_to_upload['contexts'])\n",
    "ground_truth = list(data_to_upload['ground_truth'])\n",
    "\n",
    "data_samples = {\n",
    "    'question': question,\n",
    "    'answer': answer,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': ground_truth\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb8365c821b447091f1481dc230db40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.7043, 'faithfulness': 0.4108, 'context_recall': 0.7619, 'context_precision': 0.8571}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "# from ragas.integrations.langsmith import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.to_pandas()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
