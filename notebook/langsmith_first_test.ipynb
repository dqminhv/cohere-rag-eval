{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations with LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .env file\n",
    "#pip install -U python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI()\n",
    "embedding_model=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ATLAS_CONNECTION_STRING\"] = os.getenv(\"ATLAS_CONNECTION_STRING\")\n",
    "client = MongoClient(os.environ[\"ATLAS_CONNECTION_STRING\"])\n",
    "db_name = \"tech_innovators_db\"\n",
    "collection_name = \"tech_innovators_collection\"\n",
    "atlas_collection = client[db_name][collection_name]\n",
    "index_name = \"vector_index_erp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store_retriver(index_name, embedding_model, collection):\n",
    "\n",
    "  vector_store = MongoDBAtlasVectorSearch(\n",
    "      embedding = embedding_model,\n",
    "      collection = atlas_collection,\n",
    "      index_name = index_name\n",
    "  )\n",
    "\n",
    "  retriever = vector_store.as_retriever(\n",
    "      search_type = \"similarity\",\n",
    "      search_kwargs = { \"k\": 10 }\n",
    "  )\n",
    "\n",
    "  return(vector_store, retriever)\n",
    "\n",
    "vector_store, retriever = get_vector_store_retriver(\"vector_index_erp\", embedding_model, atlas_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What information should be included in the policies and procedures section of the company's welcome message?\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one example question for the dataset for testing\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "examples = list(client.list_examples(dataset_name=\"hr test\"))\n",
    "\n",
    "q = examples[0].inputs\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langsmith\\client.py:5431: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and generate using the relevant snippets from the docs\n",
    "# Instantiate Atlas Vector Search as a retriever\n",
    "vectorstore_retriever  = vector_store.as_retriever(\n",
    "   search_type = \"similarity\",\n",
    "   search_kwargs = { \"k\": 10 }\n",
    ")\n",
    "\n",
    "# load a RAG prompt from Langchain HUB\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# our llm of choice\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def ragas_output_parser(docs):\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "generator = prompt | llm | StrOutputParser()\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {\n",
    "        \"context\": vectorstore_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "filter_langsmith_dataset = RunnableLambda(\n",
    "    lambda x: x[\"question\"] if isinstance(x, dict) else x\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": filter_langsmith_dataset,\n",
    "        \"answer\": filter_langsmith_dataset | retriever | generator,\n",
    "        \"contexts\": filter_langsmith_dataset\n",
    "        | vectorstore_retriever\n",
    "        | ragas_output_parser,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The policies and procedures section of the company's welcome message should include information on employee conduct, dress code, attendance expectations, and any other important guidelines for working at the company. It should also cover topics such as safety protocols, data security measures, and communication channels within the organization. Additionally, the policies and procedures section should outline the process for reporting any issues or concerns to the appropriate personnel.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with the example question to see if everything is working\n",
    "get_answer = RunnableLambda(lambda x: x[\"answer\"])\n",
    "resp = (rag_chain | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a test dataset in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing dataset:  hr test\n"
     ]
    }
   ],
   "source": [
    "# dataset creation\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"hr test\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    print(\"No dataset exist: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAGAS evaluation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "#from ragas.integrations.langchain import EvaluatorChain\n",
    "#from ragas.metrics import (\n",
    "#    faithfulness,\n",
    "#    answer_relevancy,\n",
    "#    context_precision,\n",
    "#    context_recall,\n",
    "#)\n",
    "\n",
    "# create evaluation chains\n",
    "#faithfulness_chain = EvaluatorChain(metric=faithfulness)\n",
    "#answer_rel_chain = EvaluatorChain(metric=answer_relevancy)\n",
    "#context_rel_chain = EvaluatorChain(metric=context_precision)\n",
    "#context_recall_chain = EvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "#evaluation_config = RunEvalConfig(\n",
    "#    custom_evaluators=[\n",
    "#        faithfulness_chain,\n",
    "#        answer_rel_chain,\n",
    "#        context_rel_chain,\n",
    "#        context_recall_chain,\n",
    "#    ],\n",
    "#    prediction_key=\"result\",\n",
    "#)\n",
    "\n",
    "#result = run_on_dataset(\n",
    "#    client,\n",
    "#    dataset_name,\n",
    "#    create_qa_chain,\n",
    "#    evaluation=evaluation_config,\n",
    "#    input_mapper=lambda x: x,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (Without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"answer\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda _: [\"\"]),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness\n",
    "from ragas.integrations.langsmith import evaluate\n",
    "\n",
    "# evaluate just llm\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm,\n",
    "    experiment_name=\"just_llm_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness\n",
    "from ragas.integrations.langsmith import evaluate\n",
    "#from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'rag_chain_5' at:\n",
      "https://smith.langchain.com/o/08bc9556-81b3-56d7-98aa-4f87d6cdfca5/datasets/f04f14f3-f165-48c3-8d94-dbf759844c7d/compare?selectedSessions=da581b53-3f5c-4dac-a714-f328ea6f9693\n",
      "\n",
      "View all tests for Dataset hr test at:\n",
      "https://smith.langchain.com/o/08bc9556-81b3-56d7-98aa-4f87d6cdfca5/datasets/f04f14f3-f165-48c3-8d94-dbf759844c7d\n",
      "[>                                                 ] 0/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run bbf5e936-5fea-436f-89e3-3e217e5a5b1d with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----->                                            ] 1/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run 8f3c4545-8bcf-414b-9b0b-12cfa58a6d16 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n",
      "Error evaluating run 7656cf2b-ea15-4316-9bcd-db0a143f43c0 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------->                                       ] 2/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n",
      "Error evaluating run 4c8eb819-b5e6-4562-a7c3-32500687d470 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Error evaluating run 57d24c78-02a1-4dbc-87f2-cb6635baf5ff with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_2'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_2'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------->                            ] 4/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_2'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------->                      ] 5/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run bc81cee9-46a8-4166-8928-24e1d6f2a6c7 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------->                 ] 6/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run 27e06bc8-ac04-47ac-bf05-31e9a4848db6 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Error evaluating run e74614ae-2ab7-4156-bcaf-68f454fec772 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------->      ] 8/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run b30a0e8d-1712-429d-9f54-e3b36face358 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 9/9"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4c8eb819-b5e6-4562-a7c3-32500687d470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.931884</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455481</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.218915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.609093</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.936864</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.215408</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.574173</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       error  execution_time                                run_id\n",
       "count      0        9.000000                                     9\n",
       "unique     0             NaN                                     9\n",
       "top      NaN             NaN  4c8eb819-b5e6-4562-a7c3-32500687d470\n",
       "freq     NaN             NaN                                     1\n",
       "mean     NaN        1.931884                                   NaN\n",
       "std      NaN        0.455481                                   NaN\n",
       "min      NaN        1.218915                                   NaN\n",
       "25%      NaN        1.609093                                   NaN\n",
       "50%      NaN        1.936864                                   NaN\n",
       "75%      NaN        2.215408                                   NaN\n",
       "max      NaN        2.574173                                   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"hr test\"\n",
    "# evaluate rag_chain\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=rag_chain,\n",
    "    experiment_name=\"rag_chain_5\",\n",
    "    #metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    reference = example.outputs[\"output_answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
