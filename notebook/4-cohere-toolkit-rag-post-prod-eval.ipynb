{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter notebook provides a framework to evaluate the performance of the Cohere toolkit RAG system using the RAGAS (Relevance, Answerability, Grammar, Accuracy, and Specificity) evaluation framework. The evaluation is based on a test dataset of questions and answers collected from a chat history extraction process.\n",
    "\n",
    "**Key Steps in the Notebook**\n",
    "\n",
    "1. **Loading Environment Variables and Dependencies**: The notebook loads environment variables from a `.env` file and installs the required dependencies, including `python-dotenv` and `langsmith`.\n",
    "2. **Loading Test Data**: The notebook loads the test dataset from a CSV file, which contains questions, answers, and contexts.\n",
    "3. **Data Preprocessing**: The notebook preprocesses the data by converting the contexts to a list format using the `serialize_list` function and saving the preprocessed data to a new CSV file.\n",
    "4. **Converting to RAGAS Format**: The notebook converts the preprocessed data to the RAGAS format using the `Dataset` class from the `datasets` library.\n",
    "5. **Setting up Langsmith Client**: The notebook sets up a Langsmith client using the `Client` class from the `langsmith` library, configuring the Langchain endpoint, project, and API key.\n",
    "6. **Running RAGAS Evaluation**: The notebook runs the RAGAS evaluation using the `evaluate` function from the `ragas` library, specifying the metrics to evaluate, including answer relevancy and faithfulness.\n",
    "7. **Saving Evaluation Results**: The notebook saves the evaluation results to a new CSV file using the `to_csv` method.\n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "The notebook evaluates the following two metrics:\n",
    "\n",
    "1. **Answer Relevancy**: Measures how well the system's answers match the intent of the question.\n",
    "2. **Faithfulness**: Measures how well the system's answers align with the context provided.\n",
    "\n",
    "**Recommendations for Future Improvements**\n",
    "\n",
    "1. **Increase Sample Size**: The notebook only evaluates the first 10 system responses. Increasing the sample size can provide a more comprehensive evaluation of the system's performance.\n",
    "2. **Add More Evaluation Metrics**: The notebook only evaluates two metrics. Adding more metrics, such as context recall and precision, can provide a more detailed evaluation of the system's performance.\n",
    "3. **Refine Evaluation Metrics**: The notebook uses predefined evaluation metrics. Refining these metrics to better align with the specific use case or application can provide more accurate evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')\n",
    "from from_root import from_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def serialize_list(value):\n",
    "    \"\"\"Serializes a list to a JSON string.\"\"\"\n",
    "    return json.dumps(value)\n",
    "\n",
    "def deserialize_list(value):\n",
    "    \"\"\"Deserializes a JSON string back into a list.\"\"\"\n",
    "    return json.loads(value)\n",
    "\n",
    "def save_dataframe_with_list_column(df, filename):\n",
    "    \"\"\"Saves a DataFrame with a list column to a CSV file, preserving the list structure.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to save.\n",
    "        filename: The name of the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the serialization function to the list column\n",
    "    df['contexts'] = df['contexts'].apply(serialize_list)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_dataframe_with_list_column(filename):\n",
    "    \"\"\"Loads a DataFrame from a CSV file, restoring the list structure.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        The loaded DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the DataFrame\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Apply the deserialization function to the list column\n",
    "    df['contexts'] = df['contexts'].apply(deserialize_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data from the chat history extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from from_root import from_root\n",
    "file_name = \"test_dataset_it_openai_deployment_post_prod.csv\"\n",
    "df_question_answer_contexts = load_dataframe_with_list_column(os.path.join(from_root(), \"data-test/test-dataset/\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_test = df_question_answer_contexts[['question', 'answer', 'contexts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to RAGAS format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let evaluate the first 10 system's responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "question = list(data_to_test['question'])\n",
    "answer = list(data_to_test['answer'])\n",
    "contexts = list(data_to_test['contexts'])\n",
    "\n",
    "data_samples = {\n",
    "    'question': question,\n",
    "    'answer': answer,\n",
    "    'contexts': contexts,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "# from ragas.integrations.langsmith import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09238de4c5b94a7bbd0d028ee19791ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"eval_result_post_prod_test_dataset_it_openai_deployment.csv\"\n",
    "json_file_path = os.path.join(from_root(), \"data-test/eval-result/\", file_name)\n",
    "result.to_pandas().to_csv(json_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
