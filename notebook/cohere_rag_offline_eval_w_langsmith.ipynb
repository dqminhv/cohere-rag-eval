{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .env file\n",
    "#pip install -U python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere container's backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"response_id\":\"e973535a-6f5f-4396-a010-82285e7d5379\",\"generation_id\":\"9c1e4f60-41a5-405d-9147-0ba61de3ea56\",\"chat_history\":[{\"role\":\"USER\",\"message\":\"What is Tech Innovators Inc.'s approach to workplace harassment?\",\"tool_plan\":null,\"tool_results\":null,\"tool_calls\":null},{\"role\":\"CHATBOT\",\"message\":\"Tech Innovators Inc. has a zero-tolerance policy towards any form of workplace harassment. We strive to foster a safe, respectful, and inclusive work environment, and any instance of harassment is taken very seriously. \\n\\nOur approach involves a detailed anti-harassment policy outlined in our employee handbook, which is accessible to all employees. This policy clearly defines what constitutes harassment, providing examples of different forms of harassment, including sexual harassment, and the consequences of such behavior. \\n\\nWe regularly conduct mandatory training sessions to educate employees on workplace harassment, ensuring everyone understands their rights and responsibilities. These sessions cover how to recognize harassment, the appropriate response, and the available resources for victims. \\n\\nAny employee who feels they have been subjected to harassment is encouraged to report the incident promptly. Victims can confidentially report harassment to their direct supervisor, the HR department, or through our whistleblower policy, ensuring their comfort and safety. \\n\\nUpon receiving a harassment complaint, Tech Innovators Inc. will promptly initiate an investigation, led by a designated investigator who is impartial to the situation. The investigation will be thorough, fair, and respect the privacy of all involved parties to the best extent possible. \\n\\nDepending on the findings, the company will take appropriate action, which may include disciplinary measures up to and including termination for the perpetrator. We also offer support and accommodations to the victim, respecting their wishes and providing any necessary assistance, which can involve collaboration with external counseling services. \\n\\nFurthermore, our policy also outlines the process for appealing any decisions or actions taken post-investigation, ensuring a transparent and fair approach. \\n\\nTech Innovators Inc. believes in creating a workplace free from harassment, and any form of inappropriate behavior is contrary to our core values. We encourage open communication and a culture where employees feel comfortable reporting any incidents, knowing they will be addressed promptly and appropriately. \\n\\nThis commitment is not just a policy but a cultural mindset ingrained within the company, regularly reinforced through our actions and leadership. We strive to ensure that every employee feels valued, respected, and safe in their workplace environment.\",\"tool_plan\":null,\"tool_results\":null,\"tool_calls\":null}],\"finish_reason\":\"COMPLETE\",\"text\":\"Tech Innovators Inc. has a zero-tolerance policy towards any form of workplace harassment. We strive to foster a safe, respectful, and inclusive work environment, and any instance of harassment is taken very seriously. \\n\\nOur approach involves a detailed anti-harassment policy outlined in our employee handbook, which is accessible to all employees. This policy clearly defines what constitutes harassment, providing examples of different forms of harassment, including sexual harassment, and the consequences of such behavior. \\n\\nWe regularly conduct mandatory training sessions to educate employees on workplace harassment, ensuring everyone understands their rights and responsibilities. These sessions cover how to recognize harassment, the appropriate response, and the available resources for victims. \\n\\nAny employee who feels they have been subjected to harassment is encouraged to report the incident promptly. Victims can confidentially report harassment to their direct supervisor, the HR department, or through our whistleblower policy, ensuring their comfort and safety. \\n\\nUpon receiving a harassment complaint, Tech Innovators Inc. will promptly initiate an investigation, led by a designated investigator who is impartial to the situation. The investigation will be thorough, fair, and respect the privacy of all involved parties to the best extent possible. \\n\\nDepending on the findings, the company will take appropriate action, which may include disciplinary measures up to and including termination for the perpetrator. We also offer support and accommodations to the victim, respecting their wishes and providing any necessary assistance, which can involve collaboration with external counseling services. \\n\\nFurthermore, our policy also outlines the process for appealing any decisions or actions taken post-investigation, ensuring a transparent and fair approach. \\n\\nTech Innovators Inc. believes in creating a workplace free from harassment, and any form of inappropriate behavior is contrary to our core values. We encourage open communication and a culture where employees feel comfortable reporting any incidents, knowing they will be addressed promptly and appropriately. \\n\\nThis commitment is not just a policy but a cultural mindset ingrained within the company, regularly reinforced through our actions and leadership. We strive to ensure that every employee feels valued, respected, and safe in their workplace environment.\",\"citations\":[],\"documents\":[],\"search_results\":[],\"search_queries\":[],\"conversation_id\":\"78c60fe1-afce-447a-859b-62d9248887bc\",\"tool_calls\":[]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import cohere\n",
    "import os\n",
    "os.environ['COHERE_API_KEY'] = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "# Define the API endpoint for streaming\n",
    "url = \"http://localhost:8000/v1/chat\"\n",
    "bearer = os.getenv('BEARER_SECRET_KEY')\n",
    "\n",
    "# Set headers\n",
    "headers = {\n",
    "    \"User-Id\": \"me\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {bearer}\",\n",
    "    \"Cohere-Stream\": \"true\",  # Enable streaming for chatbot responses\n",
    "}\n",
    "\n",
    "# Set the message to send\n",
    "message = \"What is Tech Innovators Inc.'s approach to workplace harassment?\"\n",
    "\n",
    "# Create the payload as a JSON dictionary\n",
    "data = {\"message\": message}\n",
    "\n",
    "# Send the POST request using requests\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Check for successful response\n",
    "if response.status_code == 200:\n",
    "  # Handle streaming response\n",
    "  for line in response.iter_lines():\n",
    "    # Decode the response (if necessary)\n",
    "    decoded_line = line.decode(\"utf-8\")\n",
    "    # Process the received data from the stream (print it here)\n",
    "    print(decoded_line)\n",
    "else:\n",
    "  print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG pipeline (For test purpose only!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This RAG pipeline was created to test LangSmith evaluation tools. Do not use if you are able to access Cohere toolkit backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI()\n",
    "embedding_model=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MongoDB vector database\n",
    "os.environ[\"ATLAS_CONNECTION_STRING\"] = os.getenv(\"ATLAS_CONNECTION_STRING\")\n",
    "client = MongoClient(os.environ[\"ATLAS_CONNECTION_STRING\"])\n",
    "db_name = \"tech_innovators_db\"\n",
    "collection_name = \"tech_innovators_collection\"\n",
    "atlas_collection = client[db_name][collection_name]\n",
    "index_name = \"vector_index_erp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector store and retriever\n",
    "def get_vector_store_retriver(index_name, embedding_model, collection):\n",
    "\n",
    "  vector_store = MongoDBAtlasVectorSearch(\n",
    "      embedding = embedding_model,\n",
    "      collection = atlas_collection,\n",
    "      index_name = index_name\n",
    "  )\n",
    "\n",
    "  retriever = vector_store.as_retriever(\n",
    "      search_type = \"similarity\",\n",
    "      search_kwargs = { \"k\": 10 }\n",
    "  )\n",
    "\n",
    "  return(vector_store, retriever)\n",
    "\n",
    "vector_store, retriever = get_vector_store_retriver(\"vector_index_erp\", embedding_model, atlas_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What information should be included in the policies and procedures section of the company's welcome message?\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one example question from the dataset in LangSmith for testing\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "examples = list(client.list_examples(dataset_name=\"hr test\"))\n",
    "\n",
    "q = examples[0].inputs\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langsmith\\client.py:5431: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and generate using the relevant snippets from the docs\n",
    "# Instantiate Atlas Vector Search as a retriever\n",
    "vectorstore_retriever  = vector_store.as_retriever(\n",
    "   search_type = \"similarity\",\n",
    "   search_kwargs = { \"k\": 10 }\n",
    ")\n",
    "\n",
    "# load a RAG prompt from Langchain HUB\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# our llm of choice\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def ragas_output_parser(docs):\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "generator = prompt | llm | StrOutputParser()\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {\n",
    "        \"context\": vectorstore_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "filter_langsmith_dataset = RunnableLambda(\n",
    "    lambda x: x[\"question\"] if isinstance(x, dict) else x\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": filter_langsmith_dataset,\n",
    "        \"answer\": filter_langsmith_dataset | retriever | generator,\n",
    "        \"contexts\": filter_langsmith_dataset\n",
    "        | vectorstore_retriever\n",
    "        | ragas_output_parser,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The policies and procedures section of the company's welcome message should include information on employee conduct, dress code, attendance expectations, and any other important guidelines for working at the company. It should also cover topics such as safety protocols, data security measures, and communication channels within the organization. Additionally, the policies and procedures section should outline the process for reporting any issues or concerns to the appropriate personnel.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with the example question to see if everything is working\n",
    "get_answer = RunnableLambda(lambda x: x[\"answer\"])\n",
    "resp = (rag_chain | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a test dataset in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing dataset:  hr test\n"
     ]
    }
   ],
   "source": [
    "# This cell open a test dataset from LangSmith\n",
    "# It will check whether the dataset name exist in LangSmith.\n",
    "# If there is a dataset with that name, it will load the dataset. If not, it will generate an error message\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "# Create a LangSmith client object\n",
    "client = Client()\n",
    "dataset_name = \"hr test\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    print(\"No dataset exist: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation on LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline (Without RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating the RAG pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"answer\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda _: [\"\"]),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness\n",
    "from ragas.integrations.langsmith import evaluate\n",
    "\n",
    "# evaluate just llm\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm,\n",
    "    experiment_name=\"just_llm_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness\n",
    "from ragas.integrations.langsmith import evaluate\n",
    "#from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'rag_chain_5' at:\n",
      "https://smith.langchain.com/o/08bc9556-81b3-56d7-98aa-4f87d6cdfca5/datasets/f04f14f3-f165-48c3-8d94-dbf759844c7d/compare?selectedSessions=da581b53-3f5c-4dac-a714-f328ea6f9693\n",
      "\n",
      "View all tests for Dataset hr test at:\n",
      "https://smith.langchain.com/o/08bc9556-81b3-56d7-98aa-4f87d6cdfca5/datasets/f04f14f3-f165-48c3-8d94-dbf759844c7d\n",
      "[>                                                 ] 0/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run bbf5e936-5fea-436f-89e3-3e217e5a5b1d with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----->                                            ] 1/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run 8f3c4545-8bcf-414b-9b0b-12cfa58a6d16 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n",
      "Error evaluating run 7656cf2b-ea15-4316-9bcd-db0a143f43c0 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------->                                       ] 2/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n",
      "Error evaluating run 4c8eb819-b5e6-4562-a7c3-32500687d470 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Error evaluating run 57d24c78-02a1-4dbc-87f2-cb6635baf5ff with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_2'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_2'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------->                            ] 4/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_2'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------->                      ] 5/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run bc81cee9-46a8-4166-8928-24e1d6f2a6c7 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_1'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------->                 ] 6/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run 27e06bc8-ac04-47ac-bf05-31e9a4848db6 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_0'.\")\n",
      "Error evaluating run e74614ae-2ab7-4156-bcaf-68f454fec772 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_4'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------->      ] 8/9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error evaluating run b30a0e8d-1712-429d-9f54-e3b36face358 with EvaluatorChain: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain_core\\tracers\\evaluation.py\", line 127, in _evaluate_in_project\n",
      "    evaluation_result = evaluator.evaluate_run(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 210, in evaluate_run\n",
      "    eval_output = self.invoke(chain_eval, include_run_info=True)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 164, in invoke\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\langchain\\chains\\base.py\", line 154, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\integrations\\langchain.py\", line 80, in _call\n",
      "    score = self.metric.score(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 106, in score\n",
      "    raise e\n",
      "  File \"d:\\Document\\GitHub\\cohere-rag-eval\\venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 101, in score\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dqmin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 681, in get_event_loop\n",
      "    raise RuntimeError('There is no current event loop in thread %r.'\n",
      "RuntimeError: There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\n",
      "Error in EvaluatorCallbackHandler.on_chain_end callback: RuntimeError(\"There is no current event loop in thread 'ThreadPoolExecutor-40_3'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 9/9"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4c8eb819-b5e6-4562-a7c3-32500687d470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.931884</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455481</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.218915</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.609093</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.936864</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.215408</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.574173</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       error  execution_time                                run_id\n",
       "count      0        9.000000                                     9\n",
       "unique     0             NaN                                     9\n",
       "top      NaN             NaN  4c8eb819-b5e6-4562-a7c3-32500687d470\n",
       "freq     NaN             NaN                                     1\n",
       "mean     NaN        1.931884                                   NaN\n",
       "std      NaN        0.455481                                   NaN\n",
       "min      NaN        1.218915                                   NaN\n",
       "25%      NaN        1.609093                                   NaN\n",
       "50%      NaN        1.936864                                   NaN\n",
       "75%      NaN        2.215408                                   NaN\n",
       "max      NaN        2.574173                                   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"hr test\"\n",
    "# evaluate rag_chain\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=rag_chain,\n",
    "    experiment_name=\"rag_chain_5\",\n",
    "    #metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
