{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter Notebook evaluates the performance of the OpenAi deployment within the Cohere toolkit on a test dataset for HR-related questions. The notebook loads the test data, merges it with ground truth information, converts it to the RAGAS dataset format, and evaluates the model using various metrics.\n",
    "\n",
    "**Initialization and Data Loading**\n",
    "\n",
    "1. The notebook starts by loading environment variables from a `.env` file and importing necessary libraries.\n",
    "2. It defines a function to serialize and deserialize lists to and from JSON strings, which is used to store and load the test data.\n",
    "3. The notebook loads the test dataset from a CSV file using the `load_dataframe_with_list_column` function.\n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "1. The notebook imports the ground truth information from a JSON file.\n",
    "2. It merges the test dataset with the ground truth information on the question content.\n",
    "3. The resulting dataset is converted to the RAGAS dataset format using the `Dataset.from_dict` function.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "1. The notebook evaluates the RAGAS model on the test dataset using various metrics, including:\n",
    "\t* Answer Relevancy: measures the relevance of the generated answer to the question.\n",
    "\t* Faithfulness: measures the correctness of the generated answer with respect to the ground truth.\n",
    "\t* Context Recall: measures the proportion of relevant contexts recalled by the model.\n",
    "\t* Context Precision: measures the proportion of relevant contexts among all recalled contexts.\n",
    "2. The evaluation results are stored in a `result` object and converted to a Pandas DataFrame.\n",
    "\n",
    "**Result Saving**\n",
    "\n",
    "1. The evaluation results are saved to a CSV file in the `data-test/eval-result` directory.\n",
    "\n",
    "**Langsmith Tracing (Optional)**\n",
    "\n",
    "1. The notebook has an optional section for tracing runs with Langsmith. This section can be uncommented to enable tracing.\n",
    "\n",
    "**Result**\n",
    "\n",
    "The final result is a Pandas DataFrame containing the evaluation metrics for the RAGAS model on the test dataset. The DataFrame is saved to a CSV file for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When saving the test data in the notebook 2-chat-history-extraction.ipynb, we serialized the contexts column. To load that file to a dataframe, we need a function to de-serialize it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def serialize_list(value):\n",
    "    \"\"\"Serializes a list to a JSON string.\"\"\"\n",
    "    return json.dumps(value)\n",
    "\n",
    "def deserialize_list(value):\n",
    "    \"\"\"Deserializes a JSON string back into a list.\"\"\"\n",
    "    return json.loads(value)\n",
    "\n",
    "def save_dataframe_with_list_column(df, filename):\n",
    "    \"\"\"Saves a DataFrame with a list column to a CSV file, preserving the list structure.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to save.\n",
    "        filename: The name of the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the serialization function to the list column\n",
    "    df['contexts'] = df['contexts'].apply(serialize_list)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_dataframe_with_list_column(filename):\n",
    "    \"\"\"Loads a DataFrame from a CSV file, restoring the list structure.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        The loaded DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the DataFrame\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Apply the deserialization function to the list column\n",
    "    df['contexts'] = df['contexts'].apply(deserialize_list)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data from the chat history extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from from_root import from_root\n",
    "file_name = \"test_dataset_hr_openai_deployment_test.csv\"\n",
    "df_question_answer_contexts = load_dataframe_with_list_column(os.path.join(from_root(), \"data-test/test-dataset/\", file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ground truth from the test question set \n",
    "df_ground_truth = pd.read_json(os.path.join(from_root(), \"data-test/test-dataset/\", \"test_dataset_hr.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframe with the ground_truth by the question content\n",
    "data_to_test = pd.merge(df_question_answer_contexts[['question', 'answer', 'contexts']], df_ground_truth, on='question', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to RAGAS data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert testing data to RAGAS Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "question = list(data_to_test['question'])\n",
    "answer = list(data_to_test['answer'])\n",
    "contexts = list(data_to_test['contexts'])\n",
    "ground_truth = list(data_to_test['ground_truth'])\n",
    "\n",
    "data = {\n",
    "    'question': question,\n",
    "    'answer': answer,\n",
    "    'contexts': contexts,\n",
    "    'ground_truth': ground_truth\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.\n",
    "# from langsmith import Client\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Cohere_RAG_Eval\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c0d40c1c5f449bb788c30c9693385e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.8137, 'faithfulness': 0.6284, 'context_recall': 0.4286, 'context_precision': 0.4601}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of adding a header image t...</td>\n",
       "      <td>[openai]: Adding a header image to a Confluenc...</td>\n",
       "      <td>[Description#F4F5F7In a sentence or two, descr...</td>\n",
       "      <td>Adding a header image to a Confluence space en...</td>\n",
       "      <td>0.950839</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do employee engagement and disengagement d...</td>\n",
       "      <td>[openai]: Employee engagement and disengagemen...</td>\n",
       "      <td>[and disengagement are two sides of the same c...</td>\n",
       "      <td>Employee engagement and disengagement differ i...</td>\n",
       "      <td>0.906978</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the significance of identifying growth...</td>\n",
       "      <td>[openai]: Identifying growth areas in self-ass...</td>\n",
       "      <td>[assessments are used to ensure a fair compari...</td>\n",
       "      <td>Identifying growth areas in self-assessment is...</td>\n",
       "      <td>0.988894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What services does the Employee Assistance Pro...</td>\n",
       "      <td>[openai]: The Employee Assistance Program (EAP...</td>\n",
       "      <td>[and reach out to HR with any questions or con...</td>\n",
       "      <td>Employees can contact the Employee Assistance ...</td>\n",
       "      <td>0.945168</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What training programs are offered in data sci...</td>\n",
       "      <td>[openai]: Tech Innovators Inc. offers a range ...</td>\n",
       "      <td>[experience with cloud platforms and tools.Tra...</td>\n",
       "      <td>Courses covering financial analysis, budgeting...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What resources should be added for new hires i...</td>\n",
       "      <td>[openai]: To enhance the onboarding process fo...</td>\n",
       "      <td>[Offers are extended promptly, and candidates ...</td>\n",
       "      <td>Add resources for new hires in the onboarding ...</td>\n",
       "      <td>0.930121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Tech Innovators Inc.'s approach to wor...</td>\n",
       "      <td>[openai]: Tech Innovators Inc. has a zero-tole...</td>\n",
       "      <td>[is the process for handling workplace harassm...</td>\n",
       "      <td>Tech Innovators Inc. has a zero-tolerance poli...</td>\n",
       "      <td>0.974083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the purpose of adding a header image t...   \n",
       "1  How do employee engagement and disengagement d...   \n",
       "2  What is the significance of identifying growth...   \n",
       "3  What services does the Employee Assistance Pro...   \n",
       "4  What training programs are offered in data sci...   \n",
       "5  What resources should be added for new hires i...   \n",
       "6  What is Tech Innovators Inc.'s approach to wor...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  [openai]: Adding a header image to a Confluenc...   \n",
       "1  [openai]: Employee engagement and disengagemen...   \n",
       "2  [openai]: Identifying growth areas in self-ass...   \n",
       "3  [openai]: The Employee Assistance Program (EAP...   \n",
       "4  [openai]: Tech Innovators Inc. offers a range ...   \n",
       "5  [openai]: To enhance the onboarding process fo...   \n",
       "6  [openai]: Tech Innovators Inc. has a zero-tole...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Description#F4F5F7In a sentence or two, descr...   \n",
       "1  [and disengagement are two sides of the same c...   \n",
       "2  [assessments are used to ensure a fair compari...   \n",
       "3  [and reach out to HR with any questions or con...   \n",
       "4  [experience with cloud platforms and tools.Tra...   \n",
       "5  [Offers are extended promptly, and candidates ...   \n",
       "6  [is the process for handling workplace harassm...   \n",
       "\n",
       "                                        ground_truth  answer_relevancy  \\\n",
       "0  Adding a header image to a Confluence space en...          0.950839   \n",
       "1  Employee engagement and disengagement differ i...          0.906978   \n",
       "2  Identifying growth areas in self-assessment is...          0.988894   \n",
       "3  Employees can contact the Employee Assistance ...          0.945168   \n",
       "4  Courses covering financial analysis, budgeting...          0.000000   \n",
       "5  Add resources for new hires in the onboarding ...          0.930121   \n",
       "6  Tech Innovators Inc. has a zero-tolerance poli...          0.974083   \n",
       "\n",
       "   faithfulness  context_recall  context_precision  \n",
       "0      0.071429             0.0           0.000000  \n",
       "1      0.727273             1.0           0.887500  \n",
       "2      1.000000             0.0           0.000000  \n",
       "3      0.600000             1.0           0.583333  \n",
       "4      1.000000             0.0           0.000000  \n",
       "5      0.000000             0.0           0.750000  \n",
       "6      1.000000             1.0           1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = result.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result data\n",
    "file_name = \"eval_result_dataset_hr_openai_deployment.csv\"\n",
    "result.to_pandas().to_csv(os.path.join(from_root(), \"data-test/eval-result/\", file_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
